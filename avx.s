// Code generated by command: go run main.go. DO NOT EDIT.

#include "textflag.h"

DATA iv<>+0(SB)/4, $0x6a09e667
DATA iv<>+4(SB)/4, $0xbb67ae85
DATA iv<>+8(SB)/4, $0x3c6ef372
DATA iv<>+12(SB)/4, $0xa54ff53a
DATA iv<>+16(SB)/4, $0x510e527f
DATA iv<>+20(SB)/4, $0x9b05688c
DATA iv<>+24(SB)/4, $0x1f83d9ab
DATA iv<>+28(SB)/4, $0x5be0cd19
GLOBL iv<>(SB), RODATA|NOPTR, $32

DATA rot16_shuf<>+0(SB)/1, $0x02
DATA rot16_shuf<>+1(SB)/1, $0x03
DATA rot16_shuf<>+2(SB)/1, $0x00
DATA rot16_shuf<>+3(SB)/1, $0x01
DATA rot16_shuf<>+4(SB)/1, $0x06
DATA rot16_shuf<>+5(SB)/1, $0x07
DATA rot16_shuf<>+6(SB)/1, $0x04
DATA rot16_shuf<>+7(SB)/1, $0x05
DATA rot16_shuf<>+8(SB)/1, $0x0a
DATA rot16_shuf<>+9(SB)/1, $0x0b
DATA rot16_shuf<>+10(SB)/1, $0x08
DATA rot16_shuf<>+11(SB)/1, $0x09
DATA rot16_shuf<>+12(SB)/1, $0x0e
DATA rot16_shuf<>+13(SB)/1, $0x0f
DATA rot16_shuf<>+14(SB)/1, $0x0c
DATA rot16_shuf<>+15(SB)/1, $0x0d
DATA rot16_shuf<>+16(SB)/1, $0x12
DATA rot16_shuf<>+17(SB)/1, $0x13
DATA rot16_shuf<>+18(SB)/1, $0x10
DATA rot16_shuf<>+19(SB)/1, $0x11
DATA rot16_shuf<>+20(SB)/1, $0x16
DATA rot16_shuf<>+21(SB)/1, $0x17
DATA rot16_shuf<>+22(SB)/1, $0x14
DATA rot16_shuf<>+23(SB)/1, $0x15
DATA rot16_shuf<>+24(SB)/1, $0x1a
DATA rot16_shuf<>+25(SB)/1, $0x1b
DATA rot16_shuf<>+26(SB)/1, $0x18
DATA rot16_shuf<>+27(SB)/1, $0x19
DATA rot16_shuf<>+28(SB)/1, $0x1e
DATA rot16_shuf<>+29(SB)/1, $0x1f
DATA rot16_shuf<>+30(SB)/1, $0x1c
DATA rot16_shuf<>+31(SB)/1, $0x1d
GLOBL rot16_shuf<>(SB), RODATA|NOPTR, $32

DATA rot8_shuf<>+0(SB)/1, $0x01
DATA rot8_shuf<>+1(SB)/1, $0x02
DATA rot8_shuf<>+2(SB)/1, $0x03
DATA rot8_shuf<>+3(SB)/1, $0x00
DATA rot8_shuf<>+4(SB)/1, $0x05
DATA rot8_shuf<>+5(SB)/1, $0x06
DATA rot8_shuf<>+6(SB)/1, $0x07
DATA rot8_shuf<>+7(SB)/1, $0x04
DATA rot8_shuf<>+8(SB)/1, $0x09
DATA rot8_shuf<>+9(SB)/1, $0x0a
DATA rot8_shuf<>+10(SB)/1, $0x0b
DATA rot8_shuf<>+11(SB)/1, $0x08
DATA rot8_shuf<>+12(SB)/1, $0x0d
DATA rot8_shuf<>+13(SB)/1, $0x0e
DATA rot8_shuf<>+14(SB)/1, $0x0f
DATA rot8_shuf<>+15(SB)/1, $0x0c
DATA rot8_shuf<>+16(SB)/1, $0x11
DATA rot8_shuf<>+17(SB)/1, $0x12
DATA rot8_shuf<>+18(SB)/1, $0x13
DATA rot8_shuf<>+19(SB)/1, $0x10
DATA rot8_shuf<>+20(SB)/1, $0x15
DATA rot8_shuf<>+21(SB)/1, $0x16
DATA rot8_shuf<>+22(SB)/1, $0x17
DATA rot8_shuf<>+23(SB)/1, $0x14
DATA rot8_shuf<>+24(SB)/1, $0x19
DATA rot8_shuf<>+25(SB)/1, $0x1a
DATA rot8_shuf<>+26(SB)/1, $0x1b
DATA rot8_shuf<>+27(SB)/1, $0x18
DATA rot8_shuf<>+28(SB)/1, $0x1d
DATA rot8_shuf<>+29(SB)/1, $0x1e
DATA rot8_shuf<>+30(SB)/1, $0x1f
DATA rot8_shuf<>+31(SB)/1, $0x1c
GLOBL rot8_shuf<>(SB), RODATA|NOPTR, $32

DATA block_len<>+0(SB)/4, $0x00000040
DATA block_len<>+4(SB)/4, $0x00000040
DATA block_len<>+8(SB)/4, $0x00000040
DATA block_len<>+12(SB)/4, $0x00000040
DATA block_len<>+16(SB)/4, $0x00000040
DATA block_len<>+20(SB)/4, $0x00000040
DATA block_len<>+24(SB)/4, $0x00000040
DATA block_len<>+28(SB)/4, $0x00000040
GLOBL block_len<>(SB), RODATA|NOPTR, $32

DATA zero<>+0(SB)/4, $0x00000000
DATA zero<>+4(SB)/4, $0x00000000
DATA zero<>+8(SB)/4, $0x00000000
DATA zero<>+12(SB)/4, $0x00000000
DATA zero<>+16(SB)/4, $0x00000000
DATA zero<>+20(SB)/4, $0x00000000
DATA zero<>+24(SB)/4, $0x00000000
DATA zero<>+28(SB)/4, $0x00000000
GLOBL zero<>(SB), RODATA|NOPTR, $32

DATA counter<>+0(SB)/8, $0x0000000000000000
DATA counter<>+8(SB)/8, $0x0000000000000001
DATA counter<>+16(SB)/8, $0x0000000000000002
DATA counter<>+24(SB)/8, $0x0000000000000003
DATA counter<>+32(SB)/8, $0x0000000000000004
DATA counter<>+40(SB)/8, $0x0000000000000005
DATA counter<>+48(SB)/8, $0x0000000000000006
DATA counter<>+56(SB)/8, $0x0000000000000007
GLOBL counter<>(SB), RODATA|NOPTR, $64

DATA maskO<>+0(SB)/4, $0xffffffff
DATA maskO<>+4(SB)/4, $0x00000000
DATA maskO<>+8(SB)/4, $0x00000000
DATA maskO<>+12(SB)/4, $0x00000000
DATA maskO<>+16(SB)/4, $0x00000000
DATA maskO<>+20(SB)/4, $0x00000000
DATA maskO<>+24(SB)/4, $0x00000000
DATA maskO<>+28(SB)/4, $0x00000000
DATA maskO<>+32(SB)/4, $0x00000000
DATA maskO<>+36(SB)/4, $0xffffffff
DATA maskO<>+40(SB)/4, $0x00000000
DATA maskO<>+44(SB)/4, $0x00000000
DATA maskO<>+48(SB)/4, $0x00000000
DATA maskO<>+52(SB)/4, $0x00000000
DATA maskO<>+56(SB)/4, $0x00000000
DATA maskO<>+60(SB)/4, $0x00000000
DATA maskO<>+64(SB)/4, $0x00000000
DATA maskO<>+68(SB)/4, $0x00000000
DATA maskO<>+72(SB)/4, $0xffffffff
DATA maskO<>+76(SB)/4, $0x00000000
DATA maskO<>+80(SB)/4, $0x00000000
DATA maskO<>+84(SB)/4, $0x00000000
DATA maskO<>+88(SB)/4, $0x00000000
DATA maskO<>+92(SB)/4, $0x00000000
DATA maskO<>+96(SB)/4, $0x00000000
DATA maskO<>+100(SB)/4, $0x00000000
DATA maskO<>+104(SB)/4, $0x00000000
DATA maskO<>+108(SB)/4, $0xffffffff
DATA maskO<>+112(SB)/4, $0x00000000
DATA maskO<>+116(SB)/4, $0x00000000
DATA maskO<>+120(SB)/4, $0x00000000
DATA maskO<>+124(SB)/4, $0x00000000
DATA maskO<>+128(SB)/4, $0x00000000
DATA maskO<>+132(SB)/4, $0x00000000
DATA maskO<>+136(SB)/4, $0x00000000
DATA maskO<>+140(SB)/4, $0x00000000
DATA maskO<>+144(SB)/4, $0xffffffff
DATA maskO<>+148(SB)/4, $0x00000000
DATA maskO<>+152(SB)/4, $0x00000000
DATA maskO<>+156(SB)/4, $0x00000000
DATA maskO<>+160(SB)/4, $0x00000000
DATA maskO<>+164(SB)/4, $0x00000000
DATA maskO<>+168(SB)/4, $0x00000000
DATA maskO<>+172(SB)/4, $0x00000000
DATA maskO<>+176(SB)/4, $0x00000000
DATA maskO<>+180(SB)/4, $0xffffffff
DATA maskO<>+184(SB)/4, $0x00000000
DATA maskO<>+188(SB)/4, $0x00000000
DATA maskO<>+192(SB)/4, $0x00000000
DATA maskO<>+196(SB)/4, $0x00000000
DATA maskO<>+200(SB)/4, $0x00000000
DATA maskO<>+204(SB)/4, $0x00000000
DATA maskO<>+208(SB)/4, $0x00000000
DATA maskO<>+212(SB)/4, $0x00000000
DATA maskO<>+216(SB)/4, $0xffffffff
DATA maskO<>+220(SB)/4, $0x00000000
DATA maskO<>+224(SB)/4, $0x00000000
DATA maskO<>+228(SB)/4, $0x00000000
DATA maskO<>+232(SB)/4, $0x00000000
DATA maskO<>+236(SB)/4, $0x00000000
DATA maskO<>+240(SB)/4, $0x00000000
DATA maskO<>+244(SB)/4, $0x00000000
DATA maskO<>+248(SB)/4, $0x00000000
DATA maskO<>+252(SB)/4, $0xffffffff
DATA maskO<>+256(SB)/4, $0x00000000
DATA maskO<>+260(SB)/4, $0x00000000
DATA maskO<>+264(SB)/4, $0x00000000
DATA maskO<>+268(SB)/4, $0x00000000
DATA maskO<>+272(SB)/4, $0x00000000
DATA maskO<>+276(SB)/4, $0x00000000
DATA maskO<>+280(SB)/4, $0x00000000
DATA maskO<>+284(SB)/4, $0x00000000
GLOBL maskO<>(SB), RODATA|NOPTR, $288

DATA maskP<>+0(SB)/4, $0x00000000
DATA maskP<>+4(SB)/4, $0x00000000
DATA maskP<>+8(SB)/4, $0x00000000
DATA maskP<>+12(SB)/4, $0x00000000
DATA maskP<>+16(SB)/4, $0x00000000
DATA maskP<>+20(SB)/4, $0x00000000
DATA maskP<>+24(SB)/4, $0x00000000
DATA maskP<>+28(SB)/4, $0x00000000
DATA maskP<>+32(SB)/4, $0xffffffff
DATA maskP<>+36(SB)/4, $0x00000000
DATA maskP<>+40(SB)/4, $0x00000000
DATA maskP<>+44(SB)/4, $0x00000000
DATA maskP<>+48(SB)/4, $0x00000000
DATA maskP<>+52(SB)/4, $0x00000000
DATA maskP<>+56(SB)/4, $0x00000000
DATA maskP<>+60(SB)/4, $0x00000000
DATA maskP<>+64(SB)/4, $0xffffffff
DATA maskP<>+68(SB)/4, $0xffffffff
DATA maskP<>+72(SB)/4, $0x00000000
DATA maskP<>+76(SB)/4, $0x00000000
DATA maskP<>+80(SB)/4, $0x00000000
DATA maskP<>+84(SB)/4, $0x00000000
DATA maskP<>+88(SB)/4, $0x00000000
DATA maskP<>+92(SB)/4, $0x00000000
DATA maskP<>+96(SB)/4, $0xffffffff
DATA maskP<>+100(SB)/4, $0xffffffff
DATA maskP<>+104(SB)/4, $0xffffffff
DATA maskP<>+108(SB)/4, $0x00000000
DATA maskP<>+112(SB)/4, $0x00000000
DATA maskP<>+116(SB)/4, $0x00000000
DATA maskP<>+120(SB)/4, $0x00000000
DATA maskP<>+124(SB)/4, $0x00000000
DATA maskP<>+128(SB)/4, $0xffffffff
DATA maskP<>+132(SB)/4, $0xffffffff
DATA maskP<>+136(SB)/4, $0xffffffff
DATA maskP<>+140(SB)/4, $0xffffffff
DATA maskP<>+144(SB)/4, $0x00000000
DATA maskP<>+148(SB)/4, $0x00000000
DATA maskP<>+152(SB)/4, $0x00000000
DATA maskP<>+156(SB)/4, $0x00000000
DATA maskP<>+160(SB)/4, $0xffffffff
DATA maskP<>+164(SB)/4, $0xffffffff
DATA maskP<>+168(SB)/4, $0xffffffff
DATA maskP<>+172(SB)/4, $0xffffffff
DATA maskP<>+176(SB)/4, $0xffffffff
DATA maskP<>+180(SB)/4, $0x00000000
DATA maskP<>+184(SB)/4, $0x00000000
DATA maskP<>+188(SB)/4, $0x00000000
DATA maskP<>+192(SB)/4, $0xffffffff
DATA maskP<>+196(SB)/4, $0xffffffff
DATA maskP<>+200(SB)/4, $0xffffffff
DATA maskP<>+204(SB)/4, $0xffffffff
DATA maskP<>+208(SB)/4, $0xffffffff
DATA maskP<>+212(SB)/4, $0xffffffff
DATA maskP<>+216(SB)/4, $0x00000000
DATA maskP<>+220(SB)/4, $0x00000000
DATA maskP<>+224(SB)/4, $0xffffffff
DATA maskP<>+228(SB)/4, $0xffffffff
DATA maskP<>+232(SB)/4, $0xffffffff
DATA maskP<>+236(SB)/4, $0xffffffff
DATA maskP<>+240(SB)/4, $0xffffffff
DATA maskP<>+244(SB)/4, $0xffffffff
DATA maskP<>+248(SB)/4, $0xffffffff
DATA maskP<>+252(SB)/4, $0x00000000
DATA maskP<>+256(SB)/4, $0xffffffff
DATA maskP<>+260(SB)/4, $0xffffffff
DATA maskP<>+264(SB)/4, $0xffffffff
DATA maskP<>+268(SB)/4, $0xffffffff
DATA maskP<>+272(SB)/4, $0xffffffff
DATA maskP<>+276(SB)/4, $0xffffffff
DATA maskP<>+280(SB)/4, $0xffffffff
DATA maskP<>+284(SB)/4, $0xffffffff
GLOBL maskP<>(SB), RODATA|NOPTR, $288

DATA all<>+0(SB)/4, $0xffffffff
DATA all<>+4(SB)/4, $0xffffffff
DATA all<>+8(SB)/4, $0xffffffff
DATA all<>+12(SB)/4, $0xffffffff
DATA all<>+16(SB)/4, $0xffffffff
DATA all<>+20(SB)/4, $0xffffffff
DATA all<>+24(SB)/4, $0xffffffff
DATA all<>+28(SB)/4, $0xffffffff
GLOBL all<>(SB), RODATA|NOPTR, $32

DATA chunk_end<>+0(SB)/4, $0x00000002
GLOBL chunk_end<>(SB), RODATA|NOPTR, $4

// func hashF_avx(input *[8192]byte, length uint64, counter uint64, flags uint32, out *[256]byte)
// Requires: AVX, AVX2
TEXT ·hashF_avx(SB), $632-40
	MOVQ input+0(FP), AX
	MOVQ length+8(FP), CX
	MOVQ counter+16(FP), DX
	MOVL flags+24(FP), BX
	MOVQ out+32(FP), BP

	// Skip if the length is zero
	TESTQ CX, CX
	JZ    return

	// Compute complete chunks, blocks and blen
	MOVQ CX, SI
	SHRQ $0x0a, SI
	SHLQ $0x05, SI
	SUBQ $0x01, CX
	MOVQ CX, DI
	ANDQ $0x000003c0, DI
	ANDQ $0x3f, CX
	ADDQ $0x01, CX

	// Load some params into the stack (avo improvment?)
	MOVL BX, 32(SP)
	MOVQ DX, 40(SP)
	MOVQ CX, 48(SP)

	// Set up masks for block flags and stores
	LEAQ maskO<>+0(SB), DX
	LEAQ (DX)(SI*1), DX
	LEAQ maskP<>+0(SB), R8
	LEAQ (R8)(SI*1), R8

	// Load IV into vectors
	VPBROADCASTD iv<>+0(SB), Y0
	VPBROADCASTD iv<>+4(SB), Y1
	VPBROADCASTD iv<>+8(SB), Y2
	VPBROADCASTD iv<>+12(SB), Y3
	VPBROADCASTD iv<>+16(SB), Y4
	VPBROADCASTD iv<>+20(SB), Y5
	VPBROADCASTD iv<>+24(SB), Y6
	VPBROADCASTD iv<>+28(SB), Y7

	// Build and store counter data on the stack
	VPBROADCASTQ 40(SP), Y8
	VPADDQ       counter<>+0(SB), Y8, Y8
	VPBROADCASTQ 40(SP), Y9
	VPADDQ       counter<>+32(SB), Y9, Y9
	VPUNPCKLDQ   Y9, Y8, Y10
	VPUNPCKHDQ   Y9, Y8, Y8
	VPUNPCKLDQ   Y8, Y10, Y9
	VPUNPCKHDQ   Y8, Y10, Y8
	VPERMQ       $0xd8, Y9, Y9
	VPERMQ       $0xd8, Y8, Y8
	VMOVDQU      Y9, 56(SP)
	VMOVDQU      Y8, 88(SP)

	// Set up block flags and variables for iteration
	XORQ CX, CX
	ORL  $0x01, 32(SP)

loop:
	CMPQ CX, $0x00000400
	JEQ  finalize

	// Include end flags if last block
	CMPQ CX, $0x000003c0
	JNE  round_setup
	ORL  $0x02, 32(SP)

round_setup:
	// Load and transpose message vectors
	VMOVDQU     (AX)(CX*1), Y8
	VMOVDQU     1024(AX)(CX*1), Y9
	VMOVDQU     2048(AX)(CX*1), Y10
	VMOVDQU     3072(AX)(CX*1), Y11
	VMOVDQU     4096(AX)(CX*1), Y12
	VMOVDQU     5120(AX)(CX*1), Y13
	VMOVDQU     6144(AX)(CX*1), Y14
	VMOVDQU     7168(AX)(CX*1), Y15
	VMOVDQU     Y0, (SP)
	VPUNPCKLDQ  Y9, Y8, Y0
	VPUNPCKHDQ  Y9, Y8, Y8
	VPUNPCKLDQ  Y11, Y10, Y9
	VPUNPCKHDQ  Y11, Y10, Y10
	VPUNPCKLDQ  Y13, Y12, Y11
	VPUNPCKHDQ  Y13, Y12, Y12
	VPUNPCKLDQ  Y15, Y14, Y13
	VPUNPCKHDQ  Y15, Y14, Y14
	VPUNPCKLQDQ Y9, Y0, Y15
	VPUNPCKHQDQ Y9, Y0, Y0
	VPUNPCKLQDQ Y10, Y8, Y9
	VPUNPCKHQDQ Y10, Y8, Y8
	VPUNPCKLQDQ Y13, Y11, Y10
	VPUNPCKHQDQ Y13, Y11, Y11
	VPUNPCKLQDQ Y14, Y12, Y13
	VPUNPCKHQDQ Y14, Y12, Y12
	VINSERTI128 $0x01, X10, Y15, Y14
	VPERM2I128  $0x31, Y10, Y15, Y10
	VINSERTI128 $0x01, X11, Y0, Y15
	VPERM2I128  $0x31, Y11, Y0, Y0
	VINSERTI128 $0x01, X13, Y9, Y11
	VPERM2I128  $0x31, Y13, Y9, Y9
	VINSERTI128 $0x01, X12, Y8, Y13
	VPERM2I128  $0x31, Y12, Y8, Y8
	VMOVDQU     Y14, 120(SP)
	VMOVDQU     Y15, 152(SP)
	VMOVDQU     Y11, 184(SP)
	VMOVDQU     Y13, 216(SP)
	VMOVDQU     Y10, 248(SP)
	VMOVDQU     Y0, 280(SP)
	VMOVDQU     Y9, 312(SP)
	VMOVDQU     Y8, 344(SP)
	VMOVDQU     32(AX)(CX*1), Y14
	VMOVDQU     1056(AX)(CX*1), Y15
	VMOVDQU     2080(AX)(CX*1), Y11
	VMOVDQU     3104(AX)(CX*1), Y13
	VMOVDQU     4128(AX)(CX*1), Y10
	VMOVDQU     5152(AX)(CX*1), Y0
	VMOVDQU     6176(AX)(CX*1), Y9
	VMOVDQU     7200(AX)(CX*1), Y8
	VPUNPCKLDQ  Y15, Y14, Y12
	VPUNPCKHDQ  Y15, Y14, Y14
	VPUNPCKLDQ  Y13, Y11, Y15
	VPUNPCKHDQ  Y13, Y11, Y11
	VPUNPCKLDQ  Y0, Y10, Y13
	VPUNPCKHDQ  Y0, Y10, Y0
	VPUNPCKLDQ  Y8, Y9, Y10
	VPUNPCKHDQ  Y8, Y9, Y8
	VPUNPCKLQDQ Y15, Y12, Y9
	VPUNPCKHQDQ Y15, Y12, Y12
	VPUNPCKLQDQ Y11, Y14, Y15
	VPUNPCKHQDQ Y11, Y14, Y11
	VPUNPCKLQDQ Y10, Y13, Y14
	VPUNPCKHQDQ Y10, Y13, Y10
	VPUNPCKLQDQ Y8, Y0, Y13
	VPUNPCKHQDQ Y8, Y0, Y0
	VINSERTI128 $0x01, X14, Y9, Y8
	VPERM2I128  $0x31, Y14, Y9, Y9
	VINSERTI128 $0x01, X10, Y12, Y14
	VPERM2I128  $0x31, Y10, Y12, Y10
	VINSERTI128 $0x01, X13, Y15, Y12
	VPERM2I128  $0x31, Y13, Y15, Y13
	VINSERTI128 $0x01, X0, Y11, Y15
	VPERM2I128  $0x31, Y0, Y11, Y0
	VMOVDQU     Y8, 376(SP)
	VMOVDQU     Y14, 408(SP)
	VMOVDQU     Y12, 440(SP)
	VMOVDQU     Y15, 472(SP)
	VMOVDQU     Y9, 504(SP)
	VMOVDQU     Y10, 536(SP)
	VMOVDQU     Y13, 568(SP)
	VMOVDQU     Y0, 600(SP)

	// Load constants for the round
	VMOVDQU      block_len<>+0(SB), Y0
	VPBROADCASTD 32(SP), Y8
	VPBROADCASTD iv<>+0(SB), Y9
	VPBROADCASTD iv<>+4(SB), Y10
	VPBROADCASTD iv<>+8(SB), Y11
	VPBROADCASTD iv<>+12(SB), Y12
	VMOVDQU      56(SP), Y13
	VMOVDQU      88(SP), Y14

	// Insert flag and length if last block in partial chunk
	CMPQ         CX, DI
	JNE          begin_rounds
	VPBROADCASTD chunk_end<>+0(SB), Y15
	VPAND        (DX), Y15, Y15
	VPOR         Y15, Y8, Y8
	VMOVDQU      (DX), Y15
	VPXOR        all<>+0(SB), Y15, Y15
	VPAND        Y15, Y0, Y0
	VPBROADCASTD 48(SP), Y15
	VPAND        (DX), Y15, Y15
	VPOR         Y15, Y0, Y0

begin_rounds:
	// Perform the rounds
	// Round 1
	VMOVDQU (SP), Y15
	VPADDD  120(SP), Y15, Y15
	VPADDD  184(SP), Y1, Y1
	VPADDD  248(SP), Y2, Y2
	VPADDD  312(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  152(SP), Y15, Y15
	VPADDD  216(SP), Y1, Y1
	VPADDD  280(SP), Y2, Y2
	VPADDD  344(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  376(SP), Y15, Y15
	VPADDD  440(SP), Y1, Y1
	VPADDD  504(SP), Y2, Y2
	VPADDD  568(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VMOVDQU (SP), Y15
	VPADDD  408(SP), Y15, Y15
	VPADDD  472(SP), Y1, Y1
	VPADDD  536(SP), Y2, Y2
	VPADDD  600(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4

	// Round 2
	VMOVDQU (SP), Y15
	VPADDD  184(SP), Y15, Y15
	VPADDD  216(SP), Y1, Y1
	VPADDD  344(SP), Y2, Y2
	VPADDD  248(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  312(SP), Y15, Y15
	VPADDD  440(SP), Y1, Y1
	VPADDD  120(SP), Y2, Y2
	VPADDD  536(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  152(SP), Y15, Y15
	VPADDD  504(SP), Y1, Y1
	VPADDD  408(SP), Y2, Y2
	VPADDD  600(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VMOVDQU (SP), Y15
	VPADDD  472(SP), Y15, Y15
	VPADDD  280(SP), Y1, Y1
	VPADDD  568(SP), Y2, Y2
	VPADDD  376(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4

	// Round 3
	VMOVDQU (SP), Y15
	VPADDD  216(SP), Y15, Y15
	VPADDD  440(SP), Y1, Y1
	VPADDD  536(SP), Y2, Y2
	VPADDD  344(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  248(SP), Y15, Y15
	VPADDD  504(SP), Y1, Y1
	VPADDD  184(SP), Y2, Y2
	VPADDD  568(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  312(SP), Y15, Y15
	VPADDD  408(SP), Y1, Y1
	VPADDD  472(SP), Y2, Y2
	VPADDD  376(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VMOVDQU (SP), Y15
	VPADDD  280(SP), Y15, Y15
	VPADDD  120(SP), Y1, Y1
	VPADDD  600(SP), Y2, Y2
	VPADDD  152(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4

	// Round 4
	VMOVDQU (SP), Y15
	VPADDD  440(SP), Y15, Y15
	VPADDD  504(SP), Y1, Y1
	VPADDD  568(SP), Y2, Y2
	VPADDD  536(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  344(SP), Y15, Y15
	VPADDD  408(SP), Y1, Y1
	VPADDD  216(SP), Y2, Y2
	VPADDD  600(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  248(SP), Y15, Y15
	VPADDD  472(SP), Y1, Y1
	VPADDD  280(SP), Y2, Y2
	VPADDD  152(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VMOVDQU (SP), Y15
	VPADDD  120(SP), Y15, Y15
	VPADDD  184(SP), Y1, Y1
	VPADDD  376(SP), Y2, Y2
	VPADDD  312(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4

	// Round 5
	VMOVDQU (SP), Y15
	VPADDD  504(SP), Y15, Y15
	VPADDD  408(SP), Y1, Y1
	VPADDD  600(SP), Y2, Y2
	VPADDD  568(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  536(SP), Y15, Y15
	VPADDD  472(SP), Y1, Y1
	VPADDD  440(SP), Y2, Y2
	VPADDD  376(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  344(SP), Y15, Y15
	VPADDD  280(SP), Y1, Y1
	VPADDD  120(SP), Y2, Y2
	VPADDD  312(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VMOVDQU (SP), Y15
	VPADDD  184(SP), Y15, Y15
	VPADDD  216(SP), Y1, Y1
	VPADDD  152(SP), Y2, Y2
	VPADDD  248(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4

	// Round 6
	VMOVDQU (SP), Y15
	VPADDD  408(SP), Y15, Y15
	VPADDD  472(SP), Y1, Y1
	VPADDD  376(SP), Y2, Y2
	VPADDD  600(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  568(SP), Y15, Y15
	VPADDD  280(SP), Y1, Y1
	VPADDD  504(SP), Y2, Y2
	VPADDD  152(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  536(SP), Y15, Y15
	VPADDD  120(SP), Y1, Y1
	VPADDD  184(SP), Y2, Y2
	VPADDD  248(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VMOVDQU (SP), Y15
	VPADDD  216(SP), Y15, Y15
	VPADDD  440(SP), Y1, Y1
	VPADDD  312(SP), Y2, Y2
	VPADDD  344(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4

	// Round 7
	VMOVDQU (SP), Y15
	VPADDD  472(SP), Y15, Y15
	VPADDD  280(SP), Y1, Y1
	VPADDD  152(SP), Y2, Y2
	VPADDD  376(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  600(SP), Y15, Y15
	VPADDD  120(SP), Y1, Y1
	VPADDD  408(SP), Y2, Y2
	VPADDD  312(SP), Y3, Y3
	VPADDD  Y4, Y15, Y15
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y15, Y13, Y13
	VPXOR   Y1, Y14, Y14
	VPXOR   Y2, Y0, Y0
	VPXOR   Y3, Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPADDD  Y13, Y9, Y9
	VPADDD  Y14, Y10, Y10
	VPADDD  Y0, Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPXOR   Y9, Y4, Y4
	VPXOR   Y10, Y5, Y5
	VPXOR   Y11, Y6, Y6
	VPXOR   Y12, Y7, Y7
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VMOVDQU (SP), Y15
	VPADDD  568(SP), Y15, Y15
	VPADDD  184(SP), Y1, Y1
	VPADDD  216(SP), Y2, Y2
	VPADDD  344(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot16_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot16_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x0c, Y5, Y15
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x0c, Y6, Y15
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x0c, Y7, Y15
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x0c, Y4, Y15
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y15, Y4, Y4
	VMOVDQU (SP), Y15
	VPADDD  440(SP), Y15, Y15
	VPADDD  504(SP), Y1, Y1
	VPADDD  248(SP), Y2, Y2
	VPADDD  536(SP), Y3, Y3
	VPADDD  Y5, Y15, Y15
	VPADDD  Y6, Y1, Y1
	VPADDD  Y7, Y2, Y2
	VPADDD  Y4, Y3, Y3
	VPXOR   Y15, Y8, Y8
	VPXOR   Y1, Y13, Y13
	VPXOR   Y2, Y14, Y14
	VPXOR   Y3, Y0, Y0
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y13, Y13
	VPSHUFB rot8_shuf<>+0(SB), Y14, Y14
	VPSHUFB rot8_shuf<>+0(SB), Y0, Y0
	VPADDD  Y8, Y11, Y11
	VPADDD  Y13, Y12, Y12
	VPADDD  Y14, Y9, Y9
	VPADDD  Y0, Y10, Y10
	VPXOR   Y11, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y9, Y7, Y7
	VPXOR   Y10, Y4, Y4
	VMOVDQU Y15, (SP)
	VPSRLD  $0x07, Y5, Y15
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y15, Y5, Y5
	VPSRLD  $0x07, Y6, Y15
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y15, Y6, Y6
	VPSRLD  $0x07, Y7, Y15
	VPSLLD  $0x19, Y7, Y7
	VPOR    Y15, Y7, Y7
	VPSRLD  $0x07, Y4, Y15
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y15, Y4, Y4

	// Finalize rounds
	VPXOR (SP), Y9, Y9
	VPXOR Y1, Y10, Y1
	VPXOR Y2, Y11, Y2
	VPXOR Y3, Y12, Y3
	VPXOR Y4, Y13, Y4
	VPXOR Y5, Y14, Y5
	VPXOR Y6, Y0, Y0
	VPXOR Y7, Y8, Y6

	// Save state for partial chunk if necessary
	CMPQ       CX, DI
	JNE        loop_tail
	VMOVDQU    (DX), Y7
	VPMASKMOVD Y9, Y7, (BP)
	VPMASKMOVD Y1, Y7, 32(BP)
	VPMASKMOVD Y2, Y7, 64(BP)
	VPMASKMOVD Y3, Y7, 96(BP)
	VPMASKMOVD Y4, Y7, 128(BP)
	VPMASKMOVD Y5, Y7, 160(BP)
	VPMASKMOVD Y0, Y7, 192(BP)
	VPMASKMOVD Y6, Y7, 224(BP)

	// If we have zero complete chunks, we're done
	CMPQ SI, $0x00
	JNE  loop_tail
	RET

loop_tail:
	// Fix up registers for next iteration
	VMOVDQU Y6, Y7
	VMOVDQU Y0, Y6
	VMOVDQU Y9, Y0

	// Increment, reset flags, and loop
	ADDQ $0x40, CX
	MOVL BX, 32(SP)
	JMP  loop

finalize:
	// Store prefix of full chunks into output
	VMOVDQU    (R8), Y8
	VPMASKMOVD Y0, Y8, (BP)
	VPMASKMOVD Y1, Y8, 32(BP)
	VPMASKMOVD Y2, Y8, 64(BP)
	VPMASKMOVD Y3, Y8, 96(BP)
	VPMASKMOVD Y4, Y8, 128(BP)
	VPMASKMOVD Y5, Y8, 160(BP)
	VPMASKMOVD Y6, Y8, 192(BP)
	VPMASKMOVD Y7, Y8, 224(BP)

return:
	RET

// func hashP_avx(left *[256]byte, right *[256]byte, flags uint8, out *[256]byte)
// Requires: AVX, AVX2
TEXT ·hashP_avx(SB), $40-32
	MOVQ    left+0(FP), AX
	MOVQ    right+8(FP), CX
	MOVBLZX flags+16(FP), DX
	MOVQ    out+24(FP), BX

	// Set up flags value
	ORL  $0x04, DX
	MOVL DX, 32(SP)

	// Perform the rounds
	// Round 1
	VPBROADCASTD iv<>+0(SB), Y0
	VPADDD       (AX), Y0, Y0
	VPBROADCASTD iv<>+4(SB), Y1
	VPADDD       64(AX), Y1, Y1
	VPBROADCASTD iv<>+8(SB), Y2
	VPADDD       128(AX), Y2, Y2
	VPBROADCASTD iv<>+12(SB), Y3
	VPADDD       192(AX), Y3, Y3
	VPBROADCASTD iv<>+16(SB), Y4
	VPADDD       Y4, Y0, Y0
	VPBROADCASTD iv<>+20(SB), Y5
	VPADDD       Y5, Y1, Y1
	VPBROADCASTD iv<>+24(SB), Y6
	VPADDD       Y6, Y2, Y2
	VPBROADCASTD iv<>+28(SB), Y7
	VPADDD       Y7, Y3, Y3
	VMOVDQU      zero<>+0(SB), Y8
	VPXOR        Y0, Y8, Y8
	VMOVDQU      zero<>+0(SB), Y9
	VPXOR        Y1, Y9, Y9
	VMOVDQU      block_len<>+0(SB), Y10
	VPXOR        Y2, Y10, Y10
	VPBROADCASTD 32(SP), Y11
	VPXOR        Y3, Y11, Y11
	VPSHUFB      rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB      rot16_shuf<>+0(SB), Y9, Y9
	VPSHUFB      rot16_shuf<>+0(SB), Y10, Y10
	VPSHUFB      rot16_shuf<>+0(SB), Y11, Y11
	VPBROADCASTD iv<>+0(SB), Y12
	VPADDD       Y8, Y12, Y12
	VPBROADCASTD iv<>+4(SB), Y13
	VPADDD       Y9, Y13, Y13
	VPBROADCASTD iv<>+8(SB), Y14
	VPADDD       Y10, Y14, Y14
	VPBROADCASTD iv<>+12(SB), Y15
	VPADDD       Y11, Y15, Y15
	VPXOR        Y12, Y4, Y4
	VPXOR        Y13, Y5, Y5
	VPXOR        Y14, Y6, Y6
	VPXOR        Y15, Y7, Y7
	VMOVDQU      Y0, (SP)
	VPSRLD       $0x0c, Y4, Y0
	VPSLLD       $0x14, Y4, Y4
	VPOR         Y0, Y4, Y0
	VPSRLD       $0x0c, Y5, Y4
	VPSLLD       $0x14, Y5, Y5
	VPOR         Y4, Y5, Y4
	VPSRLD       $0x0c, Y6, Y5
	VPSLLD       $0x14, Y6, Y6
	VPOR         Y5, Y6, Y5
	VPSRLD       $0x0c, Y7, Y6
	VPSLLD       $0x14, Y7, Y7
	VPOR         Y6, Y7, Y6
	VMOVDQU      (SP), Y7
	VPADDD       32(AX), Y7, Y7
	VPADDD       96(AX), Y1, Y1
	VPADDD       160(AX), Y2, Y2
	VPADDD       224(AX), Y3, Y3
	VPADDD       Y0, Y7, Y7
	VPADDD       Y4, Y1, Y1
	VPADDD       Y5, Y2, Y2
	VPADDD       Y6, Y3, Y3
	VPXOR        Y7, Y8, Y8
	VPXOR        Y1, Y9, Y9
	VPXOR        Y2, Y10, Y10
	VPXOR        Y3, Y11, Y11
	VPSHUFB      rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB      rot8_shuf<>+0(SB), Y9, Y9
	VPSHUFB      rot8_shuf<>+0(SB), Y10, Y10
	VPSHUFB      rot8_shuf<>+0(SB), Y11, Y11
	VPADDD       Y8, Y12, Y12
	VPADDD       Y9, Y13, Y13
	VPADDD       Y10, Y14, Y14
	VPADDD       Y11, Y15, Y15
	VPXOR        Y12, Y0, Y0
	VPXOR        Y13, Y4, Y4
	VPXOR        Y14, Y5, Y5
	VPXOR        Y15, Y6, Y6
	VMOVDQU      Y7, (SP)
	VPSRLD       $0x07, Y0, Y7
	VPSLLD       $0x19, Y0, Y0
	VPOR         Y7, Y0, Y0
	VPSRLD       $0x07, Y4, Y7
	VPSLLD       $0x19, Y4, Y4
	VPOR         Y7, Y4, Y4
	VPSRLD       $0x07, Y5, Y7
	VPSLLD       $0x19, Y5, Y5
	VPOR         Y7, Y5, Y5
	VPSRLD       $0x07, Y6, Y7
	VPSLLD       $0x19, Y6, Y6
	VPOR         Y7, Y6, Y6
	VMOVDQU      (SP), Y7
	VPADDD       (CX), Y7, Y7
	VPADDD       64(CX), Y1, Y1
	VPADDD       128(CX), Y2, Y2
	VPADDD       192(CX), Y3, Y3
	VPADDD       Y4, Y7, Y7
	VPADDD       Y5, Y1, Y1
	VPADDD       Y6, Y2, Y2
	VPADDD       Y0, Y3, Y3
	VPXOR        Y7, Y11, Y11
	VPXOR        Y1, Y8, Y8
	VPXOR        Y2, Y9, Y9
	VPXOR        Y3, Y10, Y10
	VPSHUFB      rot16_shuf<>+0(SB), Y11, Y11
	VPSHUFB      rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB      rot16_shuf<>+0(SB), Y9, Y9
	VPSHUFB      rot16_shuf<>+0(SB), Y10, Y10
	VPADDD       Y11, Y14, Y14
	VPADDD       Y8, Y15, Y15
	VPADDD       Y9, Y12, Y12
	VPADDD       Y10, Y13, Y13
	VPXOR        Y14, Y4, Y4
	VPXOR        Y15, Y5, Y5
	VPXOR        Y12, Y6, Y6
	VPXOR        Y13, Y0, Y0
	VMOVDQU      Y7, (SP)
	VPSRLD       $0x0c, Y4, Y7
	VPSLLD       $0x14, Y4, Y4
	VPOR         Y7, Y4, Y4
	VPSRLD       $0x0c, Y5, Y7
	VPSLLD       $0x14, Y5, Y5
	VPOR         Y7, Y5, Y5
	VPSRLD       $0x0c, Y6, Y7
	VPSLLD       $0x14, Y6, Y6
	VPOR         Y7, Y6, Y6
	VPSRLD       $0x0c, Y0, Y7
	VPSLLD       $0x14, Y0, Y0
	VPOR         Y7, Y0, Y0
	VMOVDQU      (SP), Y7
	VPADDD       32(CX), Y7, Y7
	VPADDD       96(CX), Y1, Y1
	VPADDD       160(CX), Y2, Y2
	VPADDD       224(CX), Y3, Y3
	VPADDD       Y4, Y7, Y7
	VPADDD       Y5, Y1, Y1
	VPADDD       Y6, Y2, Y2
	VPADDD       Y0, Y3, Y3
	VPXOR        Y7, Y11, Y11
	VPXOR        Y1, Y8, Y8
	VPXOR        Y2, Y9, Y9
	VPXOR        Y3, Y10, Y10
	VPSHUFB      rot8_shuf<>+0(SB), Y11, Y11
	VPSHUFB      rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB      rot8_shuf<>+0(SB), Y9, Y9
	VPSHUFB      rot8_shuf<>+0(SB), Y10, Y10
	VPADDD       Y11, Y14, Y14
	VPADDD       Y8, Y15, Y15
	VPADDD       Y9, Y12, Y12
	VPADDD       Y10, Y13, Y13
	VPXOR        Y14, Y4, Y4
	VPXOR        Y15, Y5, Y5
	VPXOR        Y12, Y6, Y6
	VPXOR        Y13, Y0, Y0
	VMOVDQU      Y7, (SP)
	VPSRLD       $0x07, Y4, Y7
	VPSLLD       $0x19, Y4, Y4
	VPOR         Y7, Y4, Y4
	VPSRLD       $0x07, Y5, Y7
	VPSLLD       $0x19, Y5, Y5
	VPOR         Y7, Y5, Y5
	VPSRLD       $0x07, Y6, Y7
	VPSLLD       $0x19, Y6, Y6
	VPOR         Y7, Y6, Y6
	VPSRLD       $0x07, Y0, Y7
	VPSLLD       $0x19, Y0, Y0
	VPOR         Y7, Y0, Y0

	// Round 2
	VMOVDQU (SP), Y7
	VPADDD  64(AX), Y7, Y7
	VPADDD  96(AX), Y1, Y1
	VPADDD  224(AX), Y2, Y2
	VPADDD  128(AX), Y3, Y3
	VPADDD  Y0, Y7, Y7
	VPADDD  Y4, Y1, Y1
	VPADDD  Y5, Y2, Y2
	VPADDD  Y6, Y3, Y3
	VPXOR   Y7, Y8, Y8
	VPXOR   Y1, Y9, Y9
	VPXOR   Y2, Y10, Y10
	VPXOR   Y3, Y11, Y11
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot16_shuf<>+0(SB), Y10, Y10
	VPSHUFB rot16_shuf<>+0(SB), Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPADDD  Y9, Y13, Y13
	VPADDD  Y10, Y14, Y14
	VPADDD  Y11, Y15, Y15
	VPXOR   Y12, Y0, Y0
	VPXOR   Y13, Y4, Y4
	VPXOR   Y14, Y5, Y5
	VPXOR   Y15, Y6, Y6
	VMOVDQU Y7, (SP)
	VPSRLD  $0x0c, Y0, Y7
	VPSLLD  $0x14, Y0, Y0
	VPOR    Y7, Y0, Y0
	VPSRLD  $0x0c, Y4, Y7
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x0c, Y5, Y7
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x0c, Y6, Y7
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y7, Y6, Y6
	VMOVDQU (SP), Y7
	VPADDD  192(AX), Y7, Y7
	VPADDD  64(CX), Y1, Y1
	VPADDD  (AX), Y2, Y2
	VPADDD  160(CX), Y3, Y3
	VPADDD  Y0, Y7, Y7
	VPADDD  Y4, Y1, Y1
	VPADDD  Y5, Y2, Y2
	VPADDD  Y6, Y3, Y3
	VPXOR   Y7, Y8, Y8
	VPXOR   Y1, Y9, Y9
	VPXOR   Y2, Y10, Y10
	VPXOR   Y3, Y11, Y11
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot8_shuf<>+0(SB), Y10, Y10
	VPSHUFB rot8_shuf<>+0(SB), Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPADDD  Y9, Y13, Y13
	VPADDD  Y10, Y14, Y14
	VPADDD  Y11, Y15, Y15
	VPXOR   Y12, Y0, Y0
	VPXOR   Y13, Y4, Y4
	VPXOR   Y14, Y5, Y5
	VPXOR   Y15, Y6, Y6
	VMOVDQU Y7, (SP)
	VPSRLD  $0x07, Y0, Y7
	VPSLLD  $0x19, Y0, Y0
	VPOR    Y7, Y0, Y0
	VPSRLD  $0x07, Y4, Y7
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x07, Y5, Y7
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x07, Y6, Y7
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y7, Y6, Y6
	VMOVDQU (SP), Y7
	VPADDD  32(AX), Y7, Y7
	VPADDD  128(CX), Y1, Y1
	VPADDD  32(CX), Y2, Y2
	VPADDD  224(CX), Y3, Y3
	VPADDD  Y4, Y7, Y7
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y0, Y3, Y3
	VPXOR   Y7, Y11, Y11
	VPXOR   Y1, Y8, Y8
	VPXOR   Y2, Y9, Y9
	VPXOR   Y3, Y10, Y10
	VPSHUFB rot16_shuf<>+0(SB), Y11, Y11
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot16_shuf<>+0(SB), Y10, Y10
	VPADDD  Y11, Y14, Y14
	VPADDD  Y8, Y15, Y15
	VPADDD  Y9, Y12, Y12
	VPADDD  Y10, Y13, Y13
	VPXOR   Y14, Y4, Y4
	VPXOR   Y15, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y13, Y0, Y0
	VMOVDQU Y7, (SP)
	VPSRLD  $0x0c, Y4, Y7
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x0c, Y5, Y7
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x0c, Y6, Y7
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y7, Y6, Y6
	VPSRLD  $0x0c, Y0, Y7
	VPSLLD  $0x14, Y0, Y0
	VPOR    Y7, Y0, Y0
	VMOVDQU (SP), Y7
	VPADDD  96(CX), Y7, Y7
	VPADDD  160(AX), Y1, Y1
	VPADDD  192(CX), Y2, Y2
	VPADDD  (CX), Y3, Y3
	VPADDD  Y4, Y7, Y7
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y0, Y3, Y3
	VPXOR   Y7, Y11, Y11
	VPXOR   Y1, Y8, Y8
	VPXOR   Y2, Y9, Y9
	VPXOR   Y3, Y10, Y10
	VPSHUFB rot8_shuf<>+0(SB), Y11, Y11
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot8_shuf<>+0(SB), Y10, Y10
	VPADDD  Y11, Y14, Y14
	VPADDD  Y8, Y15, Y15
	VPADDD  Y9, Y12, Y12
	VPADDD  Y10, Y13, Y13
	VPXOR   Y14, Y4, Y4
	VPXOR   Y15, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y13, Y0, Y0
	VMOVDQU Y7, (SP)
	VPSRLD  $0x07, Y4, Y7
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x07, Y5, Y7
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x07, Y6, Y7
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y7, Y6, Y6
	VPSRLD  $0x07, Y0, Y7
	VPSLLD  $0x19, Y0, Y0
	VPOR    Y7, Y0, Y0

	// Round 3
	VMOVDQU (SP), Y7
	VPADDD  96(AX), Y7, Y7
	VPADDD  64(CX), Y1, Y1
	VPADDD  160(CX), Y2, Y2
	VPADDD  224(AX), Y3, Y3
	VPADDD  Y0, Y7, Y7
	VPADDD  Y4, Y1, Y1
	VPADDD  Y5, Y2, Y2
	VPADDD  Y6, Y3, Y3
	VPXOR   Y7, Y8, Y8
	VPXOR   Y1, Y9, Y9
	VPXOR   Y2, Y10, Y10
	VPXOR   Y3, Y11, Y11
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot16_shuf<>+0(SB), Y10, Y10
	VPSHUFB rot16_shuf<>+0(SB), Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPADDD  Y9, Y13, Y13
	VPADDD  Y10, Y14, Y14
	VPADDD  Y11, Y15, Y15
	VPXOR   Y12, Y0, Y0
	VPXOR   Y13, Y4, Y4
	VPXOR   Y14, Y5, Y5
	VPXOR   Y15, Y6, Y6
	VMOVDQU Y7, (SP)
	VPSRLD  $0x0c, Y0, Y7
	VPSLLD  $0x14, Y0, Y0
	VPOR    Y7, Y0, Y0
	VPSRLD  $0x0c, Y4, Y7
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x0c, Y5, Y7
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x0c, Y6, Y7
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y7, Y6, Y6
	VMOVDQU (SP), Y7
	VPADDD  128(AX), Y7, Y7
	VPADDD  128(CX), Y1, Y1
	VPADDD  64(AX), Y2, Y2
	VPADDD  192(CX), Y3, Y3
	VPADDD  Y0, Y7, Y7
	VPADDD  Y4, Y1, Y1
	VPADDD  Y5, Y2, Y2
	VPADDD  Y6, Y3, Y3
	VPXOR   Y7, Y8, Y8
	VPXOR   Y1, Y9, Y9
	VPXOR   Y2, Y10, Y10
	VPXOR   Y3, Y11, Y11
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot8_shuf<>+0(SB), Y10, Y10
	VPSHUFB rot8_shuf<>+0(SB), Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPADDD  Y9, Y13, Y13
	VPADDD  Y10, Y14, Y14
	VPADDD  Y11, Y15, Y15
	VPXOR   Y12, Y0, Y0
	VPXOR   Y13, Y4, Y4
	VPXOR   Y14, Y5, Y5
	VPXOR   Y15, Y6, Y6
	VMOVDQU Y7, (SP)
	VPSRLD  $0x07, Y0, Y7
	VPSLLD  $0x19, Y0, Y0
	VPOR    Y7, Y0, Y0
	VPSRLD  $0x07, Y4, Y7
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x07, Y5, Y7
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x07, Y6, Y7
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y7, Y6, Y6
	VMOVDQU (SP), Y7
	VPADDD  192(AX), Y7, Y7
	VPADDD  32(CX), Y1, Y1
	VPADDD  96(CX), Y2, Y2
	VPADDD  (CX), Y3, Y3
	VPADDD  Y4, Y7, Y7
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y0, Y3, Y3
	VPXOR   Y7, Y11, Y11
	VPXOR   Y1, Y8, Y8
	VPXOR   Y2, Y9, Y9
	VPXOR   Y3, Y10, Y10
	VPSHUFB rot16_shuf<>+0(SB), Y11, Y11
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot16_shuf<>+0(SB), Y10, Y10
	VPADDD  Y11, Y14, Y14
	VPADDD  Y8, Y15, Y15
	VPADDD  Y9, Y12, Y12
	VPADDD  Y10, Y13, Y13
	VPXOR   Y14, Y4, Y4
	VPXOR   Y15, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y13, Y0, Y0
	VMOVDQU Y7, (SP)
	VPSRLD  $0x0c, Y4, Y7
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x0c, Y5, Y7
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x0c, Y6, Y7
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y7, Y6, Y6
	VPSRLD  $0x0c, Y0, Y7
	VPSLLD  $0x14, Y0, Y0
	VPOR    Y7, Y0, Y0
	VMOVDQU (SP), Y7
	VPADDD  160(AX), Y7, Y7
	VPADDD  (AX), Y1, Y1
	VPADDD  224(CX), Y2, Y2
	VPADDD  32(AX), Y3, Y3
	VPADDD  Y4, Y7, Y7
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y0, Y3, Y3
	VPXOR   Y7, Y11, Y11
	VPXOR   Y1, Y8, Y8
	VPXOR   Y2, Y9, Y9
	VPXOR   Y3, Y10, Y10
	VPSHUFB rot8_shuf<>+0(SB), Y11, Y11
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot8_shuf<>+0(SB), Y10, Y10
	VPADDD  Y11, Y14, Y14
	VPADDD  Y8, Y15, Y15
	VPADDD  Y9, Y12, Y12
	VPADDD  Y10, Y13, Y13
	VPXOR   Y14, Y4, Y4
	VPXOR   Y15, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y13, Y0, Y0
	VMOVDQU Y7, (SP)
	VPSRLD  $0x07, Y4, Y7
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x07, Y5, Y7
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x07, Y6, Y7
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y7, Y6, Y6
	VPSRLD  $0x07, Y0, Y7
	VPSLLD  $0x19, Y0, Y0
	VPOR    Y7, Y0, Y0

	// Round 4
	VMOVDQU (SP), Y7
	VPADDD  64(CX), Y7, Y7
	VPADDD  128(CX), Y1, Y1
	VPADDD  192(CX), Y2, Y2
	VPADDD  160(CX), Y3, Y3
	VPADDD  Y0, Y7, Y7
	VPADDD  Y4, Y1, Y1
	VPADDD  Y5, Y2, Y2
	VPADDD  Y6, Y3, Y3
	VPXOR   Y7, Y8, Y8
	VPXOR   Y1, Y9, Y9
	VPXOR   Y2, Y10, Y10
	VPXOR   Y3, Y11, Y11
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot16_shuf<>+0(SB), Y10, Y10
	VPSHUFB rot16_shuf<>+0(SB), Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPADDD  Y9, Y13, Y13
	VPADDD  Y10, Y14, Y14
	VPADDD  Y11, Y15, Y15
	VPXOR   Y12, Y0, Y0
	VPXOR   Y13, Y4, Y4
	VPXOR   Y14, Y5, Y5
	VPXOR   Y15, Y6, Y6
	VMOVDQU Y7, (SP)
	VPSRLD  $0x0c, Y0, Y7
	VPSLLD  $0x14, Y0, Y0
	VPOR    Y7, Y0, Y0
	VPSRLD  $0x0c, Y4, Y7
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x0c, Y5, Y7
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x0c, Y6, Y7
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y7, Y6, Y6
	VMOVDQU (SP), Y7
	VPADDD  224(AX), Y7, Y7
	VPADDD  32(CX), Y1, Y1
	VPADDD  96(AX), Y2, Y2
	VPADDD  224(CX), Y3, Y3
	VPADDD  Y0, Y7, Y7
	VPADDD  Y4, Y1, Y1
	VPADDD  Y5, Y2, Y2
	VPADDD  Y6, Y3, Y3
	VPXOR   Y7, Y8, Y8
	VPXOR   Y1, Y9, Y9
	VPXOR   Y2, Y10, Y10
	VPXOR   Y3, Y11, Y11
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot8_shuf<>+0(SB), Y10, Y10
	VPSHUFB rot8_shuf<>+0(SB), Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPADDD  Y9, Y13, Y13
	VPADDD  Y10, Y14, Y14
	VPADDD  Y11, Y15, Y15
	VPXOR   Y12, Y0, Y0
	VPXOR   Y13, Y4, Y4
	VPXOR   Y14, Y5, Y5
	VPXOR   Y15, Y6, Y6
	VMOVDQU Y7, (SP)
	VPSRLD  $0x07, Y0, Y7
	VPSLLD  $0x19, Y0, Y0
	VPOR    Y7, Y0, Y0
	VPSRLD  $0x07, Y4, Y7
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x07, Y5, Y7
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x07, Y6, Y7
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y7, Y6, Y6
	VMOVDQU (SP), Y7
	VPADDD  128(AX), Y7, Y7
	VPADDD  96(CX), Y1, Y1
	VPADDD  160(AX), Y2, Y2
	VPADDD  32(AX), Y3, Y3
	VPADDD  Y4, Y7, Y7
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y0, Y3, Y3
	VPXOR   Y7, Y11, Y11
	VPXOR   Y1, Y8, Y8
	VPXOR   Y2, Y9, Y9
	VPXOR   Y3, Y10, Y10
	VPSHUFB rot16_shuf<>+0(SB), Y11, Y11
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot16_shuf<>+0(SB), Y10, Y10
	VPADDD  Y11, Y14, Y14
	VPADDD  Y8, Y15, Y15
	VPADDD  Y9, Y12, Y12
	VPADDD  Y10, Y13, Y13
	VPXOR   Y14, Y4, Y4
	VPXOR   Y15, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y13, Y0, Y0
	VMOVDQU Y7, (SP)
	VPSRLD  $0x0c, Y4, Y7
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x0c, Y5, Y7
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x0c, Y6, Y7
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y7, Y6, Y6
	VPSRLD  $0x0c, Y0, Y7
	VPSLLD  $0x14, Y0, Y0
	VPOR    Y7, Y0, Y0
	VMOVDQU (SP), Y7
	VPADDD  (AX), Y7, Y7
	VPADDD  64(AX), Y1, Y1
	VPADDD  (CX), Y2, Y2
	VPADDD  192(AX), Y3, Y3
	VPADDD  Y4, Y7, Y7
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y0, Y3, Y3
	VPXOR   Y7, Y11, Y11
	VPXOR   Y1, Y8, Y8
	VPXOR   Y2, Y9, Y9
	VPXOR   Y3, Y10, Y10
	VPSHUFB rot8_shuf<>+0(SB), Y11, Y11
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot8_shuf<>+0(SB), Y10, Y10
	VPADDD  Y11, Y14, Y14
	VPADDD  Y8, Y15, Y15
	VPADDD  Y9, Y12, Y12
	VPADDD  Y10, Y13, Y13
	VPXOR   Y14, Y4, Y4
	VPXOR   Y15, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y13, Y0, Y0
	VMOVDQU Y7, (SP)
	VPSRLD  $0x07, Y4, Y7
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x07, Y5, Y7
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x07, Y6, Y7
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y7, Y6, Y6
	VPSRLD  $0x07, Y0, Y7
	VPSLLD  $0x19, Y0, Y0
	VPOR    Y7, Y0, Y0

	// Round 5
	VMOVDQU (SP), Y7
	VPADDD  128(CX), Y7, Y7
	VPADDD  32(CX), Y1, Y1
	VPADDD  224(CX), Y2, Y2
	VPADDD  192(CX), Y3, Y3
	VPADDD  Y0, Y7, Y7
	VPADDD  Y4, Y1, Y1
	VPADDD  Y5, Y2, Y2
	VPADDD  Y6, Y3, Y3
	VPXOR   Y7, Y8, Y8
	VPXOR   Y1, Y9, Y9
	VPXOR   Y2, Y10, Y10
	VPXOR   Y3, Y11, Y11
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot16_shuf<>+0(SB), Y10, Y10
	VPSHUFB rot16_shuf<>+0(SB), Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPADDD  Y9, Y13, Y13
	VPADDD  Y10, Y14, Y14
	VPADDD  Y11, Y15, Y15
	VPXOR   Y12, Y0, Y0
	VPXOR   Y13, Y4, Y4
	VPXOR   Y14, Y5, Y5
	VPXOR   Y15, Y6, Y6
	VMOVDQU Y7, (SP)
	VPSRLD  $0x0c, Y0, Y7
	VPSLLD  $0x14, Y0, Y0
	VPOR    Y7, Y0, Y0
	VPSRLD  $0x0c, Y4, Y7
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x0c, Y5, Y7
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x0c, Y6, Y7
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y7, Y6, Y6
	VMOVDQU (SP), Y7
	VPADDD  160(CX), Y7, Y7
	VPADDD  96(CX), Y1, Y1
	VPADDD  64(CX), Y2, Y2
	VPADDD  (CX), Y3, Y3
	VPADDD  Y0, Y7, Y7
	VPADDD  Y4, Y1, Y1
	VPADDD  Y5, Y2, Y2
	VPADDD  Y6, Y3, Y3
	VPXOR   Y7, Y8, Y8
	VPXOR   Y1, Y9, Y9
	VPXOR   Y2, Y10, Y10
	VPXOR   Y3, Y11, Y11
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot8_shuf<>+0(SB), Y10, Y10
	VPSHUFB rot8_shuf<>+0(SB), Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPADDD  Y9, Y13, Y13
	VPADDD  Y10, Y14, Y14
	VPADDD  Y11, Y15, Y15
	VPXOR   Y12, Y0, Y0
	VPXOR   Y13, Y4, Y4
	VPXOR   Y14, Y5, Y5
	VPXOR   Y15, Y6, Y6
	VMOVDQU Y7, (SP)
	VPSRLD  $0x07, Y0, Y7
	VPSLLD  $0x19, Y0, Y0
	VPOR    Y7, Y0, Y0
	VPSRLD  $0x07, Y4, Y7
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x07, Y5, Y7
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x07, Y6, Y7
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y7, Y6, Y6
	VMOVDQU (SP), Y7
	VPADDD  224(AX), Y7, Y7
	VPADDD  160(AX), Y1, Y1
	VPADDD  (AX), Y2, Y2
	VPADDD  192(AX), Y3, Y3
	VPADDD  Y4, Y7, Y7
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y0, Y3, Y3
	VPXOR   Y7, Y11, Y11
	VPXOR   Y1, Y8, Y8
	VPXOR   Y2, Y9, Y9
	VPXOR   Y3, Y10, Y10
	VPSHUFB rot16_shuf<>+0(SB), Y11, Y11
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot16_shuf<>+0(SB), Y10, Y10
	VPADDD  Y11, Y14, Y14
	VPADDD  Y8, Y15, Y15
	VPADDD  Y9, Y12, Y12
	VPADDD  Y10, Y13, Y13
	VPXOR   Y14, Y4, Y4
	VPXOR   Y15, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y13, Y0, Y0
	VMOVDQU Y7, (SP)
	VPSRLD  $0x0c, Y4, Y7
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x0c, Y5, Y7
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x0c, Y6, Y7
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y7, Y6, Y6
	VPSRLD  $0x0c, Y0, Y7
	VPSLLD  $0x14, Y0, Y0
	VPOR    Y7, Y0, Y0
	VMOVDQU (SP), Y7
	VPADDD  64(AX), Y7, Y7
	VPADDD  96(AX), Y1, Y1
	VPADDD  32(AX), Y2, Y2
	VPADDD  128(AX), Y3, Y3
	VPADDD  Y4, Y7, Y7
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y0, Y3, Y3
	VPXOR   Y7, Y11, Y11
	VPXOR   Y1, Y8, Y8
	VPXOR   Y2, Y9, Y9
	VPXOR   Y3, Y10, Y10
	VPSHUFB rot8_shuf<>+0(SB), Y11, Y11
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot8_shuf<>+0(SB), Y10, Y10
	VPADDD  Y11, Y14, Y14
	VPADDD  Y8, Y15, Y15
	VPADDD  Y9, Y12, Y12
	VPADDD  Y10, Y13, Y13
	VPXOR   Y14, Y4, Y4
	VPXOR   Y15, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y13, Y0, Y0
	VMOVDQU Y7, (SP)
	VPSRLD  $0x07, Y4, Y7
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x07, Y5, Y7
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x07, Y6, Y7
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y7, Y6, Y6
	VPSRLD  $0x07, Y0, Y7
	VPSLLD  $0x19, Y0, Y0
	VPOR    Y7, Y0, Y0

	// Round 6
	VMOVDQU (SP), Y7
	VPADDD  32(CX), Y7, Y7
	VPADDD  96(CX), Y1, Y1
	VPADDD  (CX), Y2, Y2
	VPADDD  224(CX), Y3, Y3
	VPADDD  Y0, Y7, Y7
	VPADDD  Y4, Y1, Y1
	VPADDD  Y5, Y2, Y2
	VPADDD  Y6, Y3, Y3
	VPXOR   Y7, Y8, Y8
	VPXOR   Y1, Y9, Y9
	VPXOR   Y2, Y10, Y10
	VPXOR   Y3, Y11, Y11
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot16_shuf<>+0(SB), Y10, Y10
	VPSHUFB rot16_shuf<>+0(SB), Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPADDD  Y9, Y13, Y13
	VPADDD  Y10, Y14, Y14
	VPADDD  Y11, Y15, Y15
	VPXOR   Y12, Y0, Y0
	VPXOR   Y13, Y4, Y4
	VPXOR   Y14, Y5, Y5
	VPXOR   Y15, Y6, Y6
	VMOVDQU Y7, (SP)
	VPSRLD  $0x0c, Y0, Y7
	VPSLLD  $0x14, Y0, Y0
	VPOR    Y7, Y0, Y0
	VPSRLD  $0x0c, Y4, Y7
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x0c, Y5, Y7
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x0c, Y6, Y7
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y7, Y6, Y6
	VMOVDQU (SP), Y7
	VPADDD  192(CX), Y7, Y7
	VPADDD  160(AX), Y1, Y1
	VPADDD  128(CX), Y2, Y2
	VPADDD  32(AX), Y3, Y3
	VPADDD  Y0, Y7, Y7
	VPADDD  Y4, Y1, Y1
	VPADDD  Y5, Y2, Y2
	VPADDD  Y6, Y3, Y3
	VPXOR   Y7, Y8, Y8
	VPXOR   Y1, Y9, Y9
	VPXOR   Y2, Y10, Y10
	VPXOR   Y3, Y11, Y11
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot8_shuf<>+0(SB), Y10, Y10
	VPSHUFB rot8_shuf<>+0(SB), Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPADDD  Y9, Y13, Y13
	VPADDD  Y10, Y14, Y14
	VPADDD  Y11, Y15, Y15
	VPXOR   Y12, Y0, Y0
	VPXOR   Y13, Y4, Y4
	VPXOR   Y14, Y5, Y5
	VPXOR   Y15, Y6, Y6
	VMOVDQU Y7, (SP)
	VPSRLD  $0x07, Y0, Y7
	VPSLLD  $0x19, Y0, Y0
	VPOR    Y7, Y0, Y0
	VPSRLD  $0x07, Y4, Y7
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x07, Y5, Y7
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x07, Y6, Y7
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y7, Y6, Y6
	VMOVDQU (SP), Y7
	VPADDD  160(CX), Y7, Y7
	VPADDD  (AX), Y1, Y1
	VPADDD  64(AX), Y2, Y2
	VPADDD  128(AX), Y3, Y3
	VPADDD  Y4, Y7, Y7
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y0, Y3, Y3
	VPXOR   Y7, Y11, Y11
	VPXOR   Y1, Y8, Y8
	VPXOR   Y2, Y9, Y9
	VPXOR   Y3, Y10, Y10
	VPSHUFB rot16_shuf<>+0(SB), Y11, Y11
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot16_shuf<>+0(SB), Y10, Y10
	VPADDD  Y11, Y14, Y14
	VPADDD  Y8, Y15, Y15
	VPADDD  Y9, Y12, Y12
	VPADDD  Y10, Y13, Y13
	VPXOR   Y14, Y4, Y4
	VPXOR   Y15, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y13, Y0, Y0
	VMOVDQU Y7, (SP)
	VPSRLD  $0x0c, Y4, Y7
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x0c, Y5, Y7
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x0c, Y6, Y7
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y7, Y6, Y6
	VPSRLD  $0x0c, Y0, Y7
	VPSLLD  $0x14, Y0, Y0
	VPOR    Y7, Y0, Y0
	VMOVDQU (SP), Y7
	VPADDD  96(AX), Y7, Y7
	VPADDD  64(CX), Y1, Y1
	VPADDD  192(AX), Y2, Y2
	VPADDD  224(AX), Y3, Y3
	VPADDD  Y4, Y7, Y7
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y0, Y3, Y3
	VPXOR   Y7, Y11, Y11
	VPXOR   Y1, Y8, Y8
	VPXOR   Y2, Y9, Y9
	VPXOR   Y3, Y10, Y10
	VPSHUFB rot8_shuf<>+0(SB), Y11, Y11
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot8_shuf<>+0(SB), Y10, Y10
	VPADDD  Y11, Y14, Y14
	VPADDD  Y8, Y15, Y15
	VPADDD  Y9, Y12, Y12
	VPADDD  Y10, Y13, Y13
	VPXOR   Y14, Y4, Y4
	VPXOR   Y15, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y13, Y0, Y0
	VMOVDQU Y7, (SP)
	VPSRLD  $0x07, Y4, Y7
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x07, Y5, Y7
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x07, Y6, Y7
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y7, Y6, Y6
	VPSRLD  $0x07, Y0, Y7
	VPSLLD  $0x19, Y0, Y0
	VPOR    Y7, Y0, Y0

	// Round 7
	VMOVDQU (SP), Y7
	VPADDD  96(CX), Y7, Y7
	VPADDD  160(AX), Y1, Y1
	VPADDD  32(AX), Y2, Y2
	VPADDD  (CX), Y3, Y3
	VPADDD  Y0, Y7, Y7
	VPADDD  Y4, Y1, Y1
	VPADDD  Y5, Y2, Y2
	VPADDD  Y6, Y3, Y3
	VPXOR   Y7, Y8, Y8
	VPXOR   Y1, Y9, Y9
	VPXOR   Y2, Y10, Y10
	VPXOR   Y3, Y11, Y11
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot16_shuf<>+0(SB), Y10, Y10
	VPSHUFB rot16_shuf<>+0(SB), Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPADDD  Y9, Y13, Y13
	VPADDD  Y10, Y14, Y14
	VPADDD  Y11, Y15, Y15
	VPXOR   Y12, Y0, Y0
	VPXOR   Y13, Y4, Y4
	VPXOR   Y14, Y5, Y5
	VPXOR   Y15, Y6, Y6
	VMOVDQU Y7, (SP)
	VPSRLD  $0x0c, Y0, Y7
	VPSLLD  $0x14, Y0, Y0
	VPOR    Y7, Y0, Y0
	VPSRLD  $0x0c, Y4, Y7
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x0c, Y5, Y7
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x0c, Y6, Y7
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y7, Y6, Y6
	VMOVDQU (SP), Y7
	VPADDD  224(CX), Y7, Y7
	VPADDD  (AX), Y1, Y1
	VPADDD  32(CX), Y2, Y2
	VPADDD  192(AX), Y3, Y3
	VPADDD  Y0, Y7, Y7
	VPADDD  Y4, Y1, Y1
	VPADDD  Y5, Y2, Y2
	VPADDD  Y6, Y3, Y3
	VPXOR   Y7, Y8, Y8
	VPXOR   Y1, Y9, Y9
	VPXOR   Y2, Y10, Y10
	VPXOR   Y3, Y11, Y11
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot8_shuf<>+0(SB), Y10, Y10
	VPSHUFB rot8_shuf<>+0(SB), Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPADDD  Y9, Y13, Y13
	VPADDD  Y10, Y14, Y14
	VPADDD  Y11, Y15, Y15
	VPXOR   Y12, Y0, Y0
	VPXOR   Y13, Y4, Y4
	VPXOR   Y14, Y5, Y5
	VPXOR   Y15, Y6, Y6
	VMOVDQU Y7, (SP)
	VPSRLD  $0x07, Y0, Y7
	VPSLLD  $0x19, Y0, Y0
	VPOR    Y7, Y0, Y0
	VPSRLD  $0x07, Y4, Y7
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x07, Y5, Y7
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x07, Y6, Y7
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y7, Y6, Y6
	VMOVDQU (SP), Y7
	VPADDD  192(CX), Y7, Y7
	VPADDD  64(AX), Y1, Y1
	VPADDD  96(AX), Y2, Y2
	VPADDD  224(AX), Y3, Y3
	VPADDD  Y4, Y7, Y7
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y0, Y3, Y3
	VPXOR   Y7, Y11, Y11
	VPXOR   Y1, Y8, Y8
	VPXOR   Y2, Y9, Y9
	VPXOR   Y3, Y10, Y10
	VPSHUFB rot16_shuf<>+0(SB), Y11, Y11
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot16_shuf<>+0(SB), Y10, Y10
	VPADDD  Y11, Y14, Y14
	VPADDD  Y8, Y15, Y15
	VPADDD  Y9, Y12, Y12
	VPADDD  Y10, Y13, Y13
	VPXOR   Y14, Y4, Y4
	VPXOR   Y15, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y13, Y0, Y0
	VMOVDQU Y7, (SP)
	VPSRLD  $0x0c, Y4, Y7
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x0c, Y5, Y7
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x0c, Y6, Y7
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y7, Y6, Y6
	VPSRLD  $0x0c, Y0, Y7
	VPSLLD  $0x14, Y0, Y0
	VPOR    Y7, Y0, Y0
	VMOVDQU (SP), Y7
	VPADDD  64(CX), Y7, Y7
	VPADDD  128(CX), Y1, Y1
	VPADDD  128(AX), Y2, Y2
	VPADDD  160(CX), Y3, Y3
	VPADDD  Y4, Y7, Y7
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y0, Y3, Y3
	VPXOR   Y7, Y11, Y11
	VPXOR   Y1, Y8, Y8
	VPXOR   Y2, Y9, Y9
	VPXOR   Y3, Y10, Y10
	VPSHUFB rot8_shuf<>+0(SB), Y11, Y11
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot8_shuf<>+0(SB), Y10, Y10
	VPADDD  Y11, Y14, Y14
	VPADDD  Y8, Y15, Y15
	VPADDD  Y9, Y12, Y12
	VPADDD  Y10, Y13, Y13
	VPXOR   Y14, Y4, Y4
	VPXOR   Y15, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y13, Y0, Y0
	VMOVDQU Y7, (SP)
	VPSRLD  $0x07, Y4, Y7
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x07, Y5, Y7
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x07, Y6, Y7
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y7, Y6, Y6
	VPSRLD  $0x07, Y0, Y7
	VPSLLD  $0x19, Y0, Y0
	VPOR    Y7, Y0, Y0

	// Finalize
	VPXOR (SP), Y12, Y7
	VPXOR Y1, Y13, Y1
	VPXOR Y2, Y14, Y2
	VPXOR Y3, Y15, Y3
	VPXOR Y0, Y8, Y0
	VPXOR Y4, Y9, Y4
	VPXOR Y5, Y10, Y5
	VPXOR Y6, Y11, Y6

	// Store result into out
	VMOVDQU Y7, (BX)
	VMOVDQU Y1, 32(BX)
	VMOVDQU Y2, 64(BX)
	VMOVDQU Y3, 96(BX)
	VMOVDQU Y0, 128(BX)
	VMOVDQU Y4, 160(BX)
	VMOVDQU Y5, 192(BX)
	VMOVDQU Y6, 224(BX)
	RET

// func movc_avx(input *[256]byte, icol uint64, out *[256]byte, ocol uint64)
// Requires: AVX, AVX2
TEXT ·movc_avx(SB), $0-32
	MOVQ         input+0(FP), AX
	MOVQ         icol+8(FP), CX
	MOVQ         out+16(FP), DX
	MOVQ         ocol+24(FP), BX
	SHLQ         $0x05, BX
	LEAQ         maskO<>+0(SB), BP
	LEAQ         (AX)(CX*4), AX
	VPBROADCASTD (AX), Y0
	VMOVDQU      (BP)(BX*1), Y1
	VPMASKMOVD   Y0, Y1, (DX)
	VPBROADCASTD 32(AX), Y0
	VPMASKMOVD   Y0, Y1, 32(DX)
	VPBROADCASTD 64(AX), Y0
	VPMASKMOVD   Y0, Y1, 64(DX)
	VPBROADCASTD 96(AX), Y0
	VPMASKMOVD   Y0, Y1, 96(DX)
	VPBROADCASTD 128(AX), Y0
	VPMASKMOVD   Y0, Y1, 128(DX)
	VPBROADCASTD 160(AX), Y0
	VPMASKMOVD   Y0, Y1, 160(DX)
	VPBROADCASTD 192(AX), Y0
	VPMASKMOVD   Y0, Y1, 192(DX)
	VPBROADCASTD 224(AX), Y0
	VPMASKMOVD   Y0, Y1, 224(DX)
	RET

// func round_avx(m *byte)
// Requires: AVX, AVX2
TEXT ·round_avx(SB), $32-8
	MOVQ    m+0(FP), AX
	VPADDD  (AX), Y0, Y0
	VPADDD  64(AX), Y1, Y1
	VPADDD  128(AX), Y2, Y2
	VPADDD  192(AX), Y3, Y3
	VPADDD  Y4, Y0, Y0
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y7, Y3, Y3
	VPXOR   Y0, Y8, Y8
	VPXOR   Y1, Y9, Y9
	VPXOR   Y2, Y10, Y10
	VPXOR   Y3, Y11, Y11
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot16_shuf<>+0(SB), Y10, Y10
	VPSHUFB rot16_shuf<>+0(SB), Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPADDD  Y9, Y13, Y13
	VPADDD  Y10, Y14, Y14
	VPADDD  Y11, Y15, Y15
	VPXOR   Y12, Y4, Y4
	VPXOR   Y13, Y5, Y5
	VPXOR   Y14, Y6, Y6
	VPXOR   Y15, Y7, Y7
	VMOVDQU Y0, (SP)
	VPSRLD  $0x0c, Y4, Y0
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y0, Y4, Y0
	VPSRLD  $0x0c, Y5, Y4
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y4, Y5, Y4
	VPSRLD  $0x0c, Y6, Y5
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y5, Y6, Y5
	VPSRLD  $0x0c, Y7, Y6
	VPSLLD  $0x14, Y7, Y7
	VPOR    Y6, Y7, Y6
	VMOVDQU (SP), Y7
	VPADDD  32(AX), Y7, Y7
	VPADDD  96(AX), Y1, Y1
	VPADDD  160(AX), Y2, Y2
	VPADDD  224(AX), Y3, Y3
	VPADDD  Y0, Y7, Y7
	VPADDD  Y4, Y1, Y1
	VPADDD  Y5, Y2, Y2
	VPADDD  Y6, Y3, Y3
	VPXOR   Y7, Y8, Y8
	VPXOR   Y1, Y9, Y9
	VPXOR   Y2, Y10, Y10
	VPXOR   Y3, Y11, Y11
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot8_shuf<>+0(SB), Y10, Y10
	VPSHUFB rot8_shuf<>+0(SB), Y11, Y11
	VPADDD  Y8, Y12, Y12
	VPADDD  Y9, Y13, Y13
	VPADDD  Y10, Y14, Y14
	VPADDD  Y11, Y15, Y15
	VPXOR   Y12, Y0, Y0
	VPXOR   Y13, Y4, Y4
	VPXOR   Y14, Y5, Y5
	VPXOR   Y15, Y6, Y6
	VMOVDQU Y7, (SP)
	VPSRLD  $0x07, Y0, Y7
	VPSLLD  $0x19, Y0, Y0
	VPOR    Y7, Y0, Y0
	VPSRLD  $0x07, Y4, Y7
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x07, Y5, Y7
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x07, Y6, Y7
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y7, Y6, Y6
	VMOVDQU (SP), Y7
	VPADDD  256(AX), Y7, Y7
	VPADDD  320(AX), Y1, Y1
	VPADDD  384(AX), Y2, Y2
	VPADDD  448(AX), Y3, Y3
	VPADDD  Y4, Y7, Y7
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y0, Y3, Y3
	VPXOR   Y7, Y11, Y11
	VPXOR   Y1, Y8, Y8
	VPXOR   Y2, Y9, Y9
	VPXOR   Y3, Y10, Y10
	VPSHUFB rot16_shuf<>+0(SB), Y11, Y11
	VPSHUFB rot16_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot16_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot16_shuf<>+0(SB), Y10, Y10
	VPADDD  Y11, Y14, Y14
	VPADDD  Y8, Y15, Y15
	VPADDD  Y9, Y12, Y12
	VPADDD  Y10, Y13, Y13
	VPXOR   Y14, Y4, Y4
	VPXOR   Y15, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y13, Y0, Y0
	VMOVDQU Y7, (SP)
	VPSRLD  $0x0c, Y4, Y7
	VPSLLD  $0x14, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x0c, Y5, Y7
	VPSLLD  $0x14, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x0c, Y6, Y7
	VPSLLD  $0x14, Y6, Y6
	VPOR    Y7, Y6, Y6
	VPSRLD  $0x0c, Y0, Y7
	VPSLLD  $0x14, Y0, Y0
	VPOR    Y7, Y0, Y0
	VMOVDQU (SP), Y7
	VPADDD  288(AX), Y7, Y7
	VPADDD  352(AX), Y1, Y1
	VPADDD  416(AX), Y2, Y2
	VPADDD  480(AX), Y3, Y3
	VPADDD  Y4, Y7, Y7
	VPADDD  Y5, Y1, Y1
	VPADDD  Y6, Y2, Y2
	VPADDD  Y0, Y3, Y3
	VPXOR   Y7, Y11, Y11
	VPXOR   Y1, Y8, Y8
	VPXOR   Y2, Y9, Y9
	VPXOR   Y3, Y10, Y10
	VPSHUFB rot8_shuf<>+0(SB), Y11, Y11
	VPSHUFB rot8_shuf<>+0(SB), Y8, Y8
	VPSHUFB rot8_shuf<>+0(SB), Y9, Y9
	VPSHUFB rot8_shuf<>+0(SB), Y10, Y10
	VPADDD  Y11, Y14, Y14
	VPADDD  Y8, Y15, Y15
	VPADDD  Y9, Y12, Y12
	VPADDD  Y10, Y13, Y13
	VPXOR   Y14, Y4, Y4
	VPXOR   Y15, Y5, Y5
	VPXOR   Y12, Y6, Y6
	VPXOR   Y13, Y0, Y0
	VMOVDQU Y7, (SP)
	VPSRLD  $0x07, Y4, Y7
	VPSLLD  $0x19, Y4, Y4
	VPOR    Y7, Y4, Y4
	VPSRLD  $0x07, Y5, Y7
	VPSLLD  $0x19, Y5, Y5
	VPOR    Y7, Y5, Y5
	VPSRLD  $0x07, Y6, Y7
	VPSLLD  $0x19, Y6, Y6
	VPOR    Y7, Y6, Y6
	VPSRLD  $0x07, Y0, Y7
	VPSLLD  $0x19, Y0, Y0
	VPOR    Y7, Y0, Y0
	RET
